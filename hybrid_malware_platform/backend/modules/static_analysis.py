# backend/modules/static_analysis.py
import os, json, math, hashlib
from pathlib import Path
import pefile
import magic

def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def file_size(path: Path) -> int:
    return path.stat().st_size

def detect_mime(path: Path) -> str:
    try:
        return magic.from_file(str(path), mime=True) or "application/octet-stream"
    except Exception:
        return "application/octet-stream"

def shannon_entropy(b: bytes) -> float:
    if not b:
        return 0.0
    freq = [0]*256
    for byte in b:
        freq[byte] += 1
    ent = 0.0
    ln = len(b)
    for c in freq:
        if c:
            p = c/ln
            ent -= p * math.log2(p)
    return round(ent, 4)

def try_pe_analysis(path: Path):
    try:
        pe = pefile.PE(str(path), fast_load=True)
    except Exception:
        return None

    results = {
        "is_pe": True,
        "machine": hex(getattr(pe.FILE_HEADER, "Machine", 0)),
        "timestamp": getattr(pe.FILE_HEADER, "TimeDateStamp", 0),
        "number_of_sections": getattr(pe.FILE_HEADER, "NumberOfSections", 0),
        "entry_point": hex(getattr(pe.OPTIONAL_HEADER, "AddressOfEntryPoint", 0)),
        "sections": [],
        "imports": {},
        "exports": [],
    }

    # Sections + entropy
    try:
        for s in pe.sections:
            name = s.Name.rstrip(b"\x00").decode(errors="ignore")
            raw_size = s.SizeOfRawData
            data = s.get_data() or b""
            ent = shannon_entropy(data)
            results["sections"].append({
                "name": name,
                "vaddr": hex(s.VirtualAddress),
                "raw_size": raw_size,
                "entropy": ent,
                "characteristics": hex(s.Characteristics),
            })
    except Exception:
        pass

    # Imports & Exports
    try:
        pe.parse_data_directories(
            directories=[
                pefile.DIRECTORY_ENTRY["IMAGE_DIRECTORY_ENTRY_IMPORT"],
                pefile.DIRECTORY_ENTRY["IMAGE_DIRECTORY_ENTRY_EXPORT"],
            ]
        )
        if hasattr(pe, "DIRECTORY_ENTRY_IMPORT"):
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                dll = entry.dll.decode(errors="ignore") if entry.dll else "UNKNOWN"
                funcs = []
                for imp in entry.imports:
                    if imp.name:
                        funcs.append(imp.name.decode(errors="ignore"))
                    else:
                        funcs.append(f"ord_{imp.ordinal}")
                results["imports"][dll] = funcs

        if hasattr(pe, "DIRECTORY_ENTRY_EXPORT") and pe.DIRECTORY_ENTRY_EXPORT:
            for sym in pe.DIRECTORY_ENTRY_EXPORT.symbols:
                name = sym.name.decode(errors="ignore") if sym.name else f"ord_{sym.ordinal}"
                results["exports"].append(name)
    except Exception:
        pass

    return results

def run_static_analysis(stored_path: str, database_dir: str) -> tuple[dict, str]:
    """
    stored_path: path to the sample saved as SHA256 (no extension)
    database_dir: folder to write JSON result as <sha256>.static.json
    """
    path = Path(stored_path)
    sha = path.name
    outdir = Path(database_dir)
    outdir.mkdir(parents=True, exist_ok=True)
    out = outdir / f"{sha}.static.json"

    info = {
        "sha256": sha,
        "size": file_size(path),
        "mime": detect_mime(path),
        "pe": None,
        "yara_matches": [],  # placeholder
        "analyzed_with": {
            "pefile": True,
            "magic": True,
            "yara": False
        }
    }

    pe_res = try_pe_analysis(path)
    if pe_res:
        info["pe"] = pe_res

    with open(out, "w", encoding="utf-8") as f:
        json.dump(info, f, indent=2)

    return info, str(out)
